{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acronyms in the data and replacing\n",
    "\n",
    "##### Get all acronyms in data and replace them with meaningful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haomiao/python_venv/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (20,21,22,23,24,39,42,43,62,63,64,67,71,72,75,76,89,90) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "data = pd.read_csv('/home/haomiao/work/target0/data/initial_cleaned_text_Airsweb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the columns that contains acronyms to list\n",
    "raw_remedial = data['Immediate remedial actions taken'].tolist()\n",
    "raw_remedial.extend(data['What were you able to do about it?'].tolist())\n",
    "raw_follow_up = data['Description of follow-up'].tolist()\n",
    "raw_incident = data['Describe the incident:'].tolist()\n",
    "raw_incident.extend(data['What did you see?'].tolist())\n",
    "raw_incident.extend(data['What could have happened?'].tolist())\n",
    "#remove nan\n",
    "cleaned_raw_remedial = [x for x in raw_remedial if str(x) != 'nan']\n",
    "cleaned_raw_follow_up = [x for x in raw_follow_up if str(x) != 'nan']\n",
    "cleaned_raw_incident = [x for x in raw_incident if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms_list = {'s&c':'S&C',\n",
    "                 's&t':'S&T',\n",
    "                 'h&s':'H&S',\n",
    "                 'L&m':'L&M',\n",
    "                 'H&s':'H&S',\n",
    "                 'e&p':'E&P',\n",
    "                 'h&S':'H&S'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace lowercase into uppercase then replace the word \n",
    "def multipleReplace(text, wordDict=acronyms_list):\n",
    "    for key in wordDict:\n",
    "        text = text.replace(key, wordDict[key])\n",
    "    return text\n",
    "cleaned_raw_follow_up= [multipleReplace(text) for text in cleaned_raw_follow_up]\n",
    "cleaned_raw_incident= [multipleReplace(text) for text in cleaned_raw_incident]\n",
    "cleaned_raw_remedial= [multipleReplace(text) for text in cleaned_raw_remedial]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get acronyms and count\n",
    "acronyms = []\n",
    "datalist = [cleaned_raw_follow_up, cleaned_raw_incident, cleaned_raw_remedial]\n",
    "#pattern = r\"[A-Z][a-zA-Z]*[A-Z]|[a-zA-Z]*\\&[a-zA-Z]|(?:[A-Z]\\.)+\"\n",
    "pattern = r\"[a-zA-Z]*\\&[a-zA-Z]*\"\n",
    "for data in datalist:\n",
    "    for text in data:\n",
    "        acronyms.extend(re.findall(pattern,text))\n",
    "acronyms_count = pd.Series(Counter(acronyms)).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms_df = acronyms_count.reset_index().assign(length = lambda x: x[\"index\"].apply(len)).sort_values(0,ascending = False).pipe(lambda x: x[(x.length<6) & (x[0]>100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms_df.to_csv('acronyms_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['&', 'S&C', 'S&T', 'H&S', 'E&P', 'E&S', 'A&E', 'D&A', 'S&CNA', 'L&M',\n",
       "       'T&C', 'A&B', 'H&M', 'L&D', 'T&V', 'W&W', 'M&E', '&T', 'S&SD', 'R&M',\n",
       "       'O&M', 'E&', 'J&A', 'R&B', 'A&D', 'N&M', 'B&B', 'P&C', 'R&C', 'D&P',\n",
       "       'S&CAN', 'CS&TE', 'D&B', '&Cold', '&Environment', '&H', 'D&As', '&RS',\n",
       "       'Consulting&Rail', 'Centre&', '&S', '&amp', 'D&G', '&b', '&collapse',\n",
       "       'A&M', '&mud', '&out', '&run', '&the', '&tools', '&very', 'A&', 'CG&U',\n",
       "       'work&spoke', 'D&UMAIN', 'W&B', 'up&dn', 'trip&', 'to&from', 'to&',\n",
       "       'shape&', 'requirements&', 'post&wire', 'o&k', 'ft&', 'employees&',\n",
       "       'ee&fcTime', 'called&', 'T&CP', 'Environment&Sustain', 'T&CE',\n",
       "       'Siemans&', 'P&P', 'M&EE', 'Luton&Dunstable', 'K&H', 'IPS&E', 'H&SM',\n",
       "       'warnings&regionName', 'H&E', 'H&', 'F&P', 'H&SAWA'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acronyms_count.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_raw_incident[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = map(nlp,cleaned_raw_incident)\n",
    "tokens_raw = [[(token.text,token.lemma_, token.tag_) for token in sent] for sent in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(chain(*tokens_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = {'VB',\n",
    "         'VBD',\n",
    "         'VBG',\n",
    "         'VBN',\n",
    "         'VBP',\n",
    "         'VBZ' }\n",
    "nouns = {'NN',\n",
    "         'NNS',\n",
    "         'NNP',\n",
    "         'NNPS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retag_word(word):\n",
    "    if word[2] in verbs:\n",
    "        return (word[0],word[1],'V')\n",
    "    elif word[2] in nouns:\n",
    "        return (word[0],word[1],'N')\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "def retag_sentence(sentence):\n",
    "    return [retag_word(word) for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retagged_words = [retag_word(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punct\n",
    "punct=[\",\",\".\",\":\",\"(\",\")\",\"-\",\"/\", \" \",\"*\",\"#\"]\n",
    "retagged_words = [i for i in retagged_words if i[0] not in punct ]\n",
    "#remove stop words\n",
    "filtered_words = [word for word in retagged_words if word[0] not in stopwords.words('english')]\n",
    "#lowercase everything\n",
    "filtered_words = [(word[0].lower(),word[1],word[2]) for word in filtered_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_nouns_incident = [i for i in filtered_words if i[2] in {\"V\",\"N\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_n_incident_df = pd.Series(Counter(word for word in verbs_nouns_incident)).sort_values(ascending = False).reset_index()\n",
    "v_n_incident_df.rename(columns ={'level_0':'word','level_1':'word_lemma','level_2':'tag',0:'count'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_incident = v_n_incident_df.loc[v_n_incident_df['tag'] == 'V'].sort(['count'],ascending = False,inplace=True)\n",
    "nouns_incident = v_n_incident_df.loc[v_n_incident_df['tag'] == 'N'].sort(['count'],ascending = False,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(Counter(word for word in filtered_words)).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagged_raw_incident = [TextBlob(' '.join(i)).tags for i in cleaned_raw_incident]\n",
    "sent = TextBlob(cleaned_raw_incident[0]).tags\n",
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_raw_incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = pd.read_csv('/home/haomiao/scratch/Target0/verbs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
